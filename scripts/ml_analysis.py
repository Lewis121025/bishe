"""
=============================================================================
åŸºäºæ•°æ®æŒ–æ˜çš„ä¸Šå¸‚å…¬å¸è´¢æ”¿è¡¥è´´ä¸é«˜ç®¡è¶…é¢è–ªé…¬ç ”ç©¶ â€” æœºå™¨å­¦ä¹ åˆ†æè„šæœ¬
=============================================================================

åˆ†æå†…å®¹ï¼š
  1. Lasso å›å½’ï¼šå˜é‡ç­›é€‰ä¸æ­£åˆ™åŒ–
  2. éšæœºæ£®æ—ï¼šè¶…é¢è–ªé…¬é¢„æµ‹ä¸ç‰¹å¾é‡è¦æ€§
  3. XGBoostï¼šæ¢¯åº¦æå‡æ¨¡å‹ä¸ SHAP å¯è§£é‡Šæ€§åˆ†æ
  4. å†³ç­–æ ‘å¯è§†åŒ–ï¼šç›´è§‚å±•ç¤ºé«˜è¶…é¢è–ªé…¬çš„æ¡ä»¶è·¯å¾„
  5. K-Means èšç±»ï¼šä¼ä¸šç”»åƒä¸æ¨¡å¼è¯†åˆ«
"""

import os
import sys
import warnings
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso, LassoCV
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree
from sklearn.cluster import KMeans
from sklearn.metrics import (r2_score, mean_squared_error, mean_absolute_error,
                              accuracy_score, classification_report, confusion_matrix,
                              silhouette_score)
import xgboost as xgb
import shap

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'Heiti TC', 'PingFang SC']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 150

# å¯¼å…¥æ•°æ®åŠ è½½å‡½æ•°
sys.path.insert(0, os.path.join(os.getcwd(), "scripts"))
from regression_analysis import (load_and_clean_data, construct_variables,
                                  filter_and_describe, compute_overpay,
                                  compute_power)


def prepare_ml_data(df):
    """å‡†å¤‡æœºå™¨å­¦ä¹ æ‰€éœ€çš„ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾"""
    print("\n" + "=" * 70)
    print("æ•°æ®å‡†å¤‡ï¼šæ„å»ºç‰¹å¾çŸ©é˜µ")
    print("=" * 70)

    # ç‰¹å¾åˆ—
    feature_cols = ["lnSubsidy", "Roa", "Lever", "Top1", "Zone", "Industry",
                    "lnSale", "IA", "Boardsize", "Dual", "Insider",
                    "Mgshder", "Tenure", "IsSOE"]

    # ç›®æ ‡å˜é‡
    target_col = "Overpay"

    # ç­›é€‰å®Œæ•´æ•°æ®
    use_cols = feature_cols + [target_col, "Symbol", "Year", "SubsidyAmount", "Top3Salary"]
    df_ml = df.dropna(subset=feature_cols + [target_col]).copy()

    print(f"  å¯ç”¨æ ·æœ¬é‡: {len(df_ml)}")
    print(f"  ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"  ç‰¹å¾åˆ—è¡¨: {feature_cols}")

    X = df_ml[feature_cols].values
    y = df_ml[target_col].values
    feature_names = feature_cols

    # æ„å»ºåˆ†ç±»æ ‡ç­¾ï¼šè¶…é¢è–ªé…¬ > 0 ä¸ºæ­£ (1)ï¼Œå¦åˆ™ä¸ºè´Ÿ (0)
    y_class = (y > 0).astype(int)
    print(f"  è¶…é¢è–ªé…¬ä¸ºæ­£çš„æ¯”ä¾‹: {y_class.mean():.2%}")

    return df_ml, X, y, y_class, feature_names


# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šLasso å›å½’ â€” å˜é‡ç­›é€‰
# ============================================================

def lasso_analysis(X, y, feature_names, output_dir):
    """Lasso å›å½’ç”¨äºå˜é‡ç­›é€‰å’Œæ­£åˆ™åŒ–"""
    print("\n" + "=" * 70)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šLasso å›å½’ â€” å˜é‡ç­›é€‰ä¸æ­£åˆ™åŒ–")
    print("=" * 70)

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # äº¤å‰éªŒè¯é€‰æ‹©æœ€ä¼˜ lambda (alpha)
    lasso_cv = LassoCV(cv=5, random_state=42, max_iter=10000)
    lasso_cv.fit(X_scaled, y)

    print(f"\n  æœ€ä¼˜æ­£åˆ™åŒ–å‚æ•° Î± = {lasso_cv.alpha_:.6f}")
    print(f"  RÂ² (äº¤å‰éªŒè¯) = {lasso_cv.score(X_scaled, y):.4f}")

    # ç³»æ•°åˆ†æ
    coefs = pd.DataFrame({
        "å˜é‡": feature_names,
        "Lassoç³»æ•°": lasso_cv.coef_
    }).sort_values("Lassoç³»æ•°", key=abs, ascending=False)

    print("\n  Lasso å›å½’ç³»æ•°ï¼ˆæŒ‰ç»å¯¹å€¼æ’åºï¼‰:")
    print("  " + "-" * 40)
    for _, row in coefs.iterrows():
        status = "âœ“ ä¿ç•™" if abs(row["Lassoç³»æ•°"]) > 1e-6 else "âœ— å‰”é™¤"
        print(f"    {row['å˜é‡']:15s}: {row['Lassoç³»æ•°']:>10.4f}  {status}")

    retained = coefs[abs(coefs["Lassoç³»æ•°"]) > 1e-6]["å˜é‡"].tolist()
    removed = coefs[abs(coefs["Lassoç³»æ•°"]) <= 1e-6]["å˜é‡"].tolist()
    print(f"\n  ä¿ç•™å˜é‡ ({len(retained)}): {retained}")
    print(f"  å‰”é™¤å˜é‡ ({len(removed)}): {removed}")

    # ç”»ç³»æ•°è·¯å¾„å›¾
    alphas = np.logspace(-5, 0, 100)
    coef_paths = []
    for alpha in alphas:
        lasso = Lasso(alpha=alpha, max_iter=10000)
        lasso.fit(X_scaled, y)
        coef_paths.append(lasso.coef_)
    coef_paths = np.array(coef_paths)

    fig, ax = plt.subplots(figsize=(10, 6))
    for i, name in enumerate(feature_names):
        ax.plot(alphas, coef_paths[:, i], label=name)
    ax.axvline(lasso_cv.alpha_, color='black', linestyle='--', label=f'æœ€ä¼˜Î±={lasso_cv.alpha_:.4f}')
    ax.set_xscale('log')
    ax.set_xlabel('æ­£åˆ™åŒ–å‚æ•° Î± (log)')
    ax.set_ylabel('Lasso å›å½’ç³»æ•°')
    ax.set_title('å›¾1  Lasso æ­£åˆ™åŒ–è·¯å¾„å›¾')
    ax.legend(fontsize=7, loc='best', ncol=2)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "fig1_lasso_path.png"), bbox_inches='tight')
    plt.close()
    print(f"\n  å·²ä¿å­˜: fig1_lasso_path.png")

    return retained


# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šéšæœºæ£®æ— â€” å›å½’é¢„æµ‹ä¸ç‰¹å¾é‡è¦æ€§
# ============================================================

def random_forest_analysis(X, y, y_class, feature_names, output_dir):
    """éšæœºæ£®æ—å›å½’ + åˆ†ç±» + ç‰¹å¾é‡è¦æ€§"""
    print("\n" + "=" * 70)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šéšæœºæ£®æ— â€” é¢„æµ‹ä¸ç‰¹å¾é‡è¦æ€§")
    print("=" * 70)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)
    _, _, yc_train, yc_test = train_test_split(
        X, y_class, test_size=0.2, random_state=42)

    # --- 2a. éšæœºæ£®æ—å›å½’ ---
    print("\n  [2a] éšæœºæ£®æ—å›å½’ï¼ˆé¢„æµ‹è¶…é¢è–ªé…¬æ•°å€¼ï¼‰")
    rf_reg = RandomForestRegressor(n_estimators=200, max_depth=10,
                                    min_samples_leaf=20, random_state=42, n_jobs=-1)
    rf_reg.fit(X_train, y_train)
    y_pred_reg = rf_reg.predict(X_test)

    r2 = r2_score(y_test, y_pred_reg)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred_reg))
    mae = mean_absolute_error(y_test, y_pred_reg)

    print(f"    RÂ² = {r2:.4f}")
    print(f"    RMSE = {rmse:.4f}")
    print(f"    MAE = {mae:.4f}")

    # äº¤å‰éªŒè¯
    cv_scores = cross_val_score(rf_reg, X, y, cv=5, scoring='r2')
    print(f"    5-fold CV RÂ² = {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

    # --- 2b. éšæœºæ£®æ—åˆ†ç±» ---
    print("\n  [2b] éšæœºæ£®æ—åˆ†ç±»ï¼ˆé¢„æµ‹æ˜¯å¦å­˜åœ¨è¶…é¢è–ªé…¬ï¼‰")
    rf_clf = RandomForestClassifier(n_estimators=200, max_depth=10,
                                     min_samples_leaf=20, random_state=42, n_jobs=-1)
    rf_clf.fit(X_train, yc_train)
    yc_pred = rf_clf.predict(X_test)

    acc = accuracy_score(yc_test, yc_pred)
    print(f"    å‡†ç¡®ç‡ = {acc:.4f}")
    print(f"\n    åˆ†ç±»æŠ¥å‘Š:")
    print(f"    {'':4}{'precision':>10}{'recall':>10}{'f1-score':>10}{'support':>10}")
    report = classification_report(yc_test, yc_pred, output_dict=True)
    for label in ['0', '1']:
        r = report[label]
        lab = "æ— è¶…é¢è–ªé…¬" if label == '0' else "æœ‰è¶…é¢è–ªé…¬"
        print(f"    {lab:8}{r['precision']:>10.4f}{r['recall']:>10.4f}{r['f1-score']:>10.4f}{r['support']:>10.0f}")
    print(f"    {'æ€»ä½“å‡†ç¡®ç‡':8}{'':>10}{'':>10}{report['accuracy']:>10.4f}{len(yc_test):>10}")

    # --- ç‰¹å¾é‡è¦æ€§ ---
    importance_reg = pd.DataFrame({
        "å˜é‡": feature_names,
        "å›å½’é‡è¦æ€§": rf_reg.feature_importances_
    }).sort_values("å›å½’é‡è¦æ€§", ascending=False)

    importance_clf = pd.DataFrame({
        "å˜é‡": feature_names,
        "åˆ†ç±»é‡è¦æ€§": rf_clf.feature_importances_
    }).sort_values("åˆ†ç±»é‡è¦æ€§", ascending=False)

    print("\n  ç‰¹å¾é‡è¦æ€§æ’åï¼ˆéšæœºæ£®æ—å›å½’ï¼‰:")
    print("  " + "-" * 35)
    for i, (_, row) in enumerate(importance_reg.iterrows()):
        bar = "â–ˆ" * int(row["å›å½’é‡è¦æ€§"] * 50)
        print(f"    {i+1:2d}. {row['å˜é‡']:15s}: {row['å›å½’é‡è¦æ€§']:.4f} {bar}")

    # ç”»ç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # å›å½’
    imp_reg_sorted = importance_reg.sort_values("å›å½’é‡è¦æ€§", ascending=True)
    axes[0].barh(imp_reg_sorted["å˜é‡"], imp_reg_sorted["å›å½’é‡è¦æ€§"], color='steelblue')
    axes[0].set_title(f'(a) éšæœºæ£®æ—å›å½’ç‰¹å¾é‡è¦æ€§\nRÂ²={r2:.4f}')
    axes[0].set_xlabel('é‡è¦æ€§')

    # åˆ†ç±»
    imp_clf_sorted = importance_clf.sort_values("åˆ†ç±»é‡è¦æ€§", ascending=True)
    axes[1].barh(imp_clf_sorted["å˜é‡"], imp_clf_sorted["åˆ†ç±»é‡è¦æ€§"], color='coral')
    axes[1].set_title(f'(b) éšæœºæ£®æ—åˆ†ç±»ç‰¹å¾é‡è¦æ€§\nAccuracy={acc:.4f}')
    axes[1].set_xlabel('é‡è¦æ€§')

    plt.suptitle('å›¾2  éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§åˆ†æ', fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "fig2_rf_importance.png"), bbox_inches='tight')
    plt.close()
    print(f"\n  å·²ä¿å­˜: fig2_rf_importance.png")

    return rf_reg, rf_clf, importance_reg


# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šXGBoost + SHAP å¯è§£é‡Šæ€§
# ============================================================

def xgboost_shap_analysis(X, y, feature_names, output_dir):
    """XGBoost å›å½’ + SHAP å€¼åˆ†æ"""
    print("\n" + "=" * 70)
    print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šXGBoost + SHAP å¯è§£é‡Šæ€§åˆ†æ")
    print("=" * 70)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    # XGBoost å›å½’
    xgb_model = xgb.XGBRegressor(
        n_estimators=300, max_depth=6, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8,
        min_child_weight=10, random_state=42, n_jobs=-1
    )
    xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)],
                   verbose=False)

    y_pred = xgb_model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    print(f"\n  XGBoost å›å½’ç»“æœ:")
    print(f"    RÂ² = {r2:.4f}")
    print(f"    RMSE = {rmse:.4f}")

    cv_scores = cross_val_score(xgb_model, X, y, cv=5, scoring='r2')
    print(f"    5-fold CV RÂ² = {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

    # SHAP åˆ†æ
    print("\n  è®¡ç®— SHAP å€¼...")
    explainer = shap.TreeExplainer(xgb_model)
    # ç”¨æµ‹è¯•é›†çš„ä¸€ä¸ªå­é›†æ¥è®¡ç®— SHAPï¼ˆåŠ é€Ÿï¼‰
    sample_size = min(2000, len(X_test))
    X_sample = X_test[:sample_size]
    shap_values = explainer.shap_values(X_sample)

    # SHAP é‡è¦æ€§æ’å
    shap_importance = np.abs(shap_values).mean(axis=0)
    shap_df = pd.DataFrame({
        "å˜é‡": feature_names,
        "SHAPå‡å€¼": shap_importance
    }).sort_values("SHAPå‡å€¼", ascending=False)

    print("\n  SHAP ç‰¹å¾é‡è¦æ€§æ’å:")
    print("  " + "-" * 35)
    for i, (_, row) in enumerate(shap_df.iterrows()):
        bar = "â–ˆ" * int(row["SHAPå‡å€¼"] / shap_df["SHAPå‡å€¼"].max() * 30)
        print(f"    {i+1:2d}. {row['å˜é‡']:15s}: {row['SHAPå‡å€¼']:.4f} {bar}")

    # SHAP Summary Plot
    fig, ax = plt.subplots(figsize=(10, 7))
    shap.summary_plot(shap_values, X_sample, feature_names=feature_names,
                       show=False, max_display=14)
    plt.title('å›¾3  SHAP ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆXGBoostï¼‰', fontsize=14, pad=20)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "fig3_shap_summary.png"), bbox_inches='tight')
    plt.close()
    print(f"\n  å·²ä¿å­˜: fig3_shap_summary.png")

    # SHAP åˆ†æ lnSubsidy çš„è¾¹é™…æ•ˆåº”
    fig, ax = plt.subplots(figsize=(8, 5))
    subsidy_idx = feature_names.index("lnSubsidy")
    shap.dependence_plot(subsidy_idx, shap_values, X_sample,
                          feature_names=feature_names, show=False, ax=ax)
    ax.set_title('å›¾4  æ”¿åºœè¡¥åŠ©(lnSubsidy)çš„SHAPä¾èµ–å›¾', fontsize=13)
    ax.set_xlabel('lnSubsidy (æ”¿åºœè¡¥åŠ©å¯¹æ•°)')
    ax.set_ylabel('SHAPå€¼ (å¯¹è¶…é¢è–ªé…¬çš„è¾¹é™…å½±å“)')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "fig4_shap_subsidy.png"), bbox_inches='tight')
    plt.close()
    print(f"  å·²ä¿å­˜: fig4_shap_subsidy.png")

    return xgb_model, shap_df


# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå†³ç­–æ ‘å¯è§†åŒ–
# ============================================================

def decision_tree_analysis(X, y_class, feature_names, output_dir):
    """å†³ç­–æ ‘åˆ†ç±» â€” å¯è§†åŒ–è¶…é¢è–ªé…¬é£é™©è·¯å¾„"""
    print("\n" + "=" * 70)
    print("ç¬¬å››éƒ¨åˆ†ï¼šå†³ç­–æ ‘ â€” è¶…é¢è–ªé…¬é£é™©è·¯å¾„")
    print("=" * 70)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_class, test_size=0.2, random_state=42)

    # æµ…å±‚å†³ç­–æ ‘ï¼ˆä¾¿äºå¯è§†åŒ–ï¼‰
    dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=100,
                                 random_state=42)
    dt.fit(X_train, y_train)

    acc = dt.score(X_test, y_test)
    print(f"\n  å†³ç­–æ ‘å‡†ç¡®ç‡: {acc:.4f}")

    # æ–‡æœ¬è§„åˆ™
    tree_rules = export_text(dt, feature_names=feature_names, max_depth=4)
    print(f"\n  å†³ç­–æ ‘è§„åˆ™:")
    for line in tree_rules.split('\n')[:20]:
        print(f"    {line}")
    if len(tree_rules.split('\n')) > 20:
        print(f"    ... (å…± {len(tree_rules.split(chr(10)))} è¡Œ)")

    # å¯è§†åŒ–
    cn = ['æ— è¶…é¢è–ªé…¬', 'æœ‰è¶…é¢è–ªé…¬']
    fig, ax = plt.subplots(figsize=(24, 10))
    plot_tree(dt, feature_names=feature_names, class_names=cn,
              filled=True, rounded=True, fontsize=8, ax=ax,
              proportion=True, impurity=False)
    ax.set_title('å›¾5  å†³ç­–æ ‘ï¼šé«˜ç®¡è¶…é¢è–ªé…¬é£é™©åˆ†ç±»è·¯å¾„', fontsize=16, pad=20)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "fig5_decision_tree.png"), bbox_inches='tight')
    plt.close()
    print(f"\n  å·²ä¿å­˜: fig5_decision_tree.png")

    return dt


# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šK-Means èšç±» â€” ä¼ä¸šç”»åƒ
# ============================================================

def kmeans_analysis(df_ml, feature_names, output_dir):
    """K-Means èšç±»åˆ†æ â€” ä¼ä¸šåˆ†ç¾¤ç”»åƒ"""
    print("\n" + "=" * 70)
    print("ç¬¬äº”éƒ¨åˆ†ï¼šK-Means èšç±» â€” ä¼ä¸šç”»åƒåˆ†æ")
    print("=" * 70)

    cluster_features = ["lnSubsidy", "Overpay", "Roa", "Lever", "Top1", "lnSale"]
    df_cluster = df_ml.dropna(subset=cluster_features).copy()

    scaler = StandardScaler()
    X_cluster = scaler.fit_transform(df_cluster[cluster_features])

    # è‚˜éƒ¨æ³•åˆ™é€‰ K
    print("\n  è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä¼˜ K:")
    inertias = []
    sil_scores = []
    K_range = range(2, 8)
    for k in K_range:
        km = KMeans(n_clusters=k, random_state=42, n_init=10)
        km.fit(X_cluster)
        inertias.append(km.inertia_)
        sil = silhouette_score(X_cluster, km.labels_, sample_size=5000)
        sil_scores.append(sil)
        print(f"    K={k}: è½®å»“ç³»æ•°={sil:.4f}, æƒ¯æ€§={km.inertia_:.0f}")

    best_k = list(K_range)[np.argmax(sil_scores)]
    print(f"\n  æœ€ä¼˜Kï¼ˆè½®å»“ç³»æ•°æœ€å¤§ï¼‰: K={best_k}")

    # ä½¿ç”¨æœ€ä¼˜ K èšç±»
    km_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    df_cluster["Cluster"] = km_final.fit_predict(X_cluster)

    # å„ç±»ç‰¹å¾å‡å€¼
    print(f"\n  å„èšç±»ä¸­å¿ƒç‰¹å¾å‡å€¼:")
    print("  " + "-" * 70)
    cluster_summary = df_cluster.groupby("Cluster")[cluster_features].mean()
    print(cluster_summary.to_string(float_format=lambda x: f"{x:.4f}"))

    # å„ç±»æ ·æœ¬é‡å’Œä¼ä¸šæ€§è´¨åˆ†å¸ƒ
    print(f"\n  å„èšç±»æ ·æœ¬é‡åŠå›½æœ‰å æ¯”:")
    for c in range(best_k):
        subset = df_cluster[df_cluster["Cluster"] == c]
        soe_pct = subset["IsSOE"].mean() * 100 if "IsSOE" in subset.columns else 0
        print(f"    èšç±»{c}: {len(subset)} å®¶  |  å›½æœ‰å æ¯”: {soe_pct:.1f}%  |  "
              f"å¹³å‡è¡¥åŠ©: {subset['SubsidyAmount'].mean():.0f}  |  "
              f"å¹³å‡è–ªé…¬: {subset['Top3Salary'].mean():.0f}")

    # ç”»èšç±»æ•£ç‚¹å›¾
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    colors = ['#2196F3', '#FF5722', '#4CAF50', '#FF9800', '#9C27B0', '#795548'][:best_k]
    cluster_names = [f'èšç±»{i}' for i in range(best_k)]

    # è¡¥åŠ©-è¶…é¢è–ªé…¬
    for c in range(best_k):
        mask = df_cluster["Cluster"] == c
        axes[0].scatter(df_cluster.loc[mask, "lnSubsidy"],
                        df_cluster.loc[mask, "Overpay"],
                        c=colors[c], label=cluster_names[c], alpha=0.3, s=5)
    axes[0].set_xlabel('lnSubsidy (æ”¿åºœè¡¥åŠ©å¯¹æ•°)')
    axes[0].set_ylabel('Overpay (è¶…é¢è–ªé…¬)')
    axes[0].set_title('(a) èšç±»åˆ†å¸ƒï¼šè¡¥åŠ© vs è¶…é¢è–ªé…¬')
    axes[0].legend()

    # è§„æ¨¡-æ æ†
    for c in range(best_k):
        mask = df_cluster["Cluster"] == c
        axes[1].scatter(df_cluster.loc[mask, "lnSale"],
                        df_cluster.loc[mask, "Lever"],
                        c=colors[c], label=cluster_names[c], alpha=0.3, s=5)
    axes[1].set_xlabel('lnSale (è¥ä¸šæ”¶å…¥å¯¹æ•°)')
    axes[1].set_ylabel('Lever (è´¢åŠ¡æ æ†)')
    axes[1].set_title('(b) èšç±»åˆ†å¸ƒï¼šè§„æ¨¡ vs æ æ†')
    axes[1].legend()

    plt.suptitle('å›¾6  K-Meansä¼ä¸šèšç±»åˆ†æ', fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "fig6_kmeans_clusters.png"), bbox_inches='tight')
    plt.close()
    print(f"\n  å·²ä¿å­˜: fig6_kmeans_clusters.png")

    # ç”»èšç±»ä¸­å¿ƒçƒ­åŠ›å›¾
    fig, ax = plt.subplots(figsize=(10, 5))
    # æ ‡å‡†åŒ–åçš„èšç±»ä¸­å¿ƒ
    centers_std = pd.DataFrame(
        scaler.transform(cluster_summary.values),
        columns=cluster_features,
        index=[f'èšç±»{i}' for i in range(best_k)]
    )
    sns.heatmap(centers_std, annot=True, fmt='.2f', cmap='RdYlBu_r',
                center=0, ax=ax, linewidths=0.5)
    ax.set_title('å›¾7  èšç±»ä¸­å¿ƒç‰¹å¾çƒ­åŠ›å›¾ï¼ˆæ ‡å‡†åŒ–ï¼‰', fontsize=14, pad=10)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "fig7_cluster_heatmap.png"), bbox_inches='tight')
    plt.close()
    print(f"  å·²ä¿å­˜: fig7_cluster_heatmap.png")

    return km_final, df_cluster


# ============================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šæ¨¡å‹å¯¹æ¯”æ€»ç»“
# ============================================================

def model_comparison(X, y, y_class, feature_names, output_dir):
    """å„æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨"""
    print("\n" + "=" * 70)
    print("ç¬¬å…­éƒ¨åˆ†ï¼šæ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("=" * 70)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    _, _, yc_train, yc_test = train_test_split(X, y_class, test_size=0.2, random_state=42)

    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)

    results = []

    # 1. Lasso
    lasso = LassoCV(cv=5, random_state=42, max_iter=10000)
    lasso.fit(X_train_s, y_train)
    y_pred = lasso.predict(X_test_s)
    results.append({"æ¨¡å‹": "Lassoå›å½’", "RÂ²": r2_score(y_test, y_pred),
                     "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
                     "MAE": mean_absolute_error(y_test, y_pred)})

    # 2. éšæœºæ£®æ—
    rf = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=20,
                                random_state=42, n_jobs=-1)
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)
    results.append({"æ¨¡å‹": "éšæœºæ£®æ—", "RÂ²": r2_score(y_test, y_pred),
                     "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
                     "MAE": mean_absolute_error(y_test, y_pred)})

    # 3. XGBoost
    xgb_m = xgb.XGBRegressor(n_estimators=300, max_depth=6, learning_rate=0.05,
                               subsample=0.8, colsample_bytree=0.8,
                               min_child_weight=10, random_state=42, n_jobs=-1)
    xgb_m.fit(X_train, y_train, verbose=False)
    y_pred = xgb_m.predict(X_test)
    results.append({"æ¨¡å‹": "XGBoost", "RÂ²": r2_score(y_test, y_pred),
                     "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
                     "MAE": mean_absolute_error(y_test, y_pred)})

    # å¯¹æ¯”è¡¨
    results_df = pd.DataFrame(results)
    print("\n  è¡¨4  æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("  " + "=" * 50)
    print(results_df.to_string(index=False, float_format=lambda x: f"{x:.4f}"))
    print("  " + "=" * 50)

    # ä¿å­˜
    results_df.to_csv(os.path.join(output_dir, "model_comparison.csv"), index=False)

    # ç”»å¯¹æ¯”æŸ±çŠ¶å›¾
    fig, axes = plt.subplots(1, 3, figsize=(14, 5))
    models = results_df["æ¨¡å‹"].tolist()
    colors = ['#2196F3', '#4CAF50', '#FF5722']

    for i, metric in enumerate(["RÂ²", "RMSE", "MAE"]):
        vals = results_df[metric].tolist()
        bars = axes[i].bar(models, vals, color=colors)
        axes[i].set_title(metric, fontsize=14)
        axes[i].set_ylabel(metric)
        for bar, val in zip(bars, vals):
            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                        f'{val:.4f}', ha='center', va='bottom', fontsize=10)

    plt.suptitle('å›¾8  å›å½’æ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "fig8_model_comparison.png"), bbox_inches='tight')
    plt.close()
    print(f"\n  å·²ä¿å­˜: fig8_model_comparison.png")

    return results_df


# ============================================================
# ä¸»æµç¨‹
# ============================================================

def main():
    data_dir = os.path.join(os.getcwd(), "processed_data")
    output_dir = os.path.join(os.getcwd(), "results")
    os.makedirs(output_dir, exist_ok=True)

    # åŠ è½½ä¸å‡†å¤‡æ•°æ®ï¼ˆå¤ç”¨ regression_analysis.py é€»è¾‘ï¼‰
    print("åŠ è½½æ•°æ®...")
    df = load_and_clean_data(data_dir)
    df = construct_variables(df)
    df = filter_and_describe(df)
    df = compute_overpay(df)
    df = compute_power(df)

    # å‡†å¤‡ ML æ•°æ®
    df_ml, X, y, y_class, feature_names = prepare_ml_data(df)

    print("\n" + "#" * 70)
    print("#  æ•°æ®æŒ–æ˜åˆ†æå¼€å§‹")
    print("#" * 70)

    # 1. Lasso å˜é‡ç­›é€‰
    retained_vars = lasso_analysis(X, y, feature_names, output_dir)

    # 2. éšæœºæ£®æ—
    rf_reg, rf_clf, rf_importance = random_forest_analysis(X, y, y_class, feature_names, output_dir)

    # 3. XGBoost + SHAP
    xgb_model, shap_importance = xgboost_shap_analysis(X, y, feature_names, output_dir)

    # 4. å†³ç­–æ ‘
    dt_model = decision_tree_analysis(X, y_class, feature_names, output_dir)

    # 5. K-Means èšç±»
    km_model, df_clustered = kmeans_analysis(df_ml, feature_names, output_dir)

    # 6. æ¨¡å‹å¯¹æ¯”
    comparison = model_comparison(X, y, y_class, feature_names, output_dir)

    # æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 70)
    print("æ•°æ®æŒ–æ˜åˆ†æå®Œæˆï¼")
    print("=" * 70)
    print(f"\nç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶ (å‡åœ¨ results/ ç›®å½•ä¸‹):")
    for f in sorted(os.listdir(output_dir)):
        if f.startswith("fig"):
            size = os.path.getsize(os.path.join(output_dir, f)) / 1024
            print(f"  ğŸ“Š {f}  ({size:.0f} KB)")

    print(f"\næ•°æ®æ–‡ä»¶:")
    print(f"  ğŸ“„ model_comparison.csv")
    print(f"  ğŸ“„ regression_dataset.csv")
    print(f"  ğŸ“„ regression_tables.txt")


if __name__ == "__main__":
    main()
